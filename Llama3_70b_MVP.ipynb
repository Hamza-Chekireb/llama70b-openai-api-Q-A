{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Hamza-Chekireb/llama70b-openai-api-Q-A/blob/main/Llama3_70b_MVP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZHqp8NyXZnkq"
      },
      "source": [
        "### 1.Import Requirements"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install openai"
      ],
      "metadata": {
        "id": "bjkIhb_Ta6sP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8FPcXjaBZ8pE"
      },
      "outputs": [],
      "source": [
        "!pip install langchain\n",
        "!pip install pypdf\n",
        "!pip install sentence_transformers\n",
        "!pip install langchain\n",
        "!pip install langchain_community\n",
        "!pip install chromadb\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JF96i95R2RMw"
      },
      "outputs": [],
      "source": [
        "from IPython.display import display, Markdown\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain.document_loaders import PyPDFLoader, DirectoryLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.document_loaders import PyPDFDirectoryLoader\n",
        "from langchain.embeddings import HuggingFaceInstructEmbeddings\n",
        "from transformers import AutoTokenizer, TextStreamer, pipeline\n",
        "from langchain import HuggingFacePipeline\n",
        "import torch\n",
        "import transformers\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline # AutoModelForCausalLM is used to add layer for application like QA\n",
        "import langchain\n",
        "from langchain import HuggingFacePipeline\n",
        "from openai import OpenAI"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OMzBTfBqzIk7"
      },
      "source": [
        "### 2.Creat the Vectore Data Base"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nVziF2Y5rjq_"
      },
      "outputs": [],
      "source": [
        "DEVICE = \"cuda:0\" if torch.cuda.is_available() else \"cpu\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tEweeyeZZyOd"
      },
      "outputs": [],
      "source": [
        "DATA_PATH = '/content/'\n",
        "DB_FAISS_PATH = 'vectorstore/db_chroma'\n",
        "\n",
        "# Create vector database\n",
        "def create_vector_db():\n",
        "    loader = DirectoryLoader(DATA_PATH,\n",
        "                             glob='*.pdf',\n",
        "                             loader_cls=PyPDFLoader)\n",
        "\n",
        "    documents = loader.load()\n",
        "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=500,\n",
        "                                                   chunk_overlap=50)\n",
        "    texts = text_splitter.split_documents(documents)\n",
        "    # embeddings = HuggingFaceInstructEmbeddings(\n",
        "    # model_name=\"hkunlp/instructor-large\", model_kwargs={\"device\": DEVICE}\n",
        "# )\n",
        "\n",
        "    embeddings = HuggingFaceEmbeddings(model_name='sentence-transformers/all-MiniLM-L6-v2',\n",
        "                                       model_kwargs={'device': DEVICE})\n",
        "\n",
        "    # 4. Creat Vector Data Base\n",
        "    db = Chroma.from_documents(texts,embeddings,persist_directory=DB_FAISS_PATH)\n",
        "    # store the db in repertory\n",
        "    db.persist()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    create_vector_db()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ttr1wtFNy72w"
      },
      "source": [
        "### 4.Output Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U73P6P-dh0ed"
      },
      "outputs": [],
      "source": [
        "# Create new connection to VDB\n",
        "\n",
        "embeddings = HuggingFaceEmbeddings(model_name='sentence-transformers/all-MiniLM-L6-v2',\n",
        "                                       model_kwargs={'device': DEVICE})\n",
        "VDBPath = 'vectorstore/db_chroma'\n",
        "vdb_connection = Chroma(persist_directory=VDBPath,  embedding_function=embeddings)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"\"\n",
        "context = \"\"\n",
        "promp_template_for_processing = f\"Answer this Question ({question}) in an exhaustive and concise way, based only on the given context.\\n\\n\" \\\n",
        "+ f\"Context: {context}\\n\\n\" + f\"Question : {question}\\n\""
      ],
      "metadata": {
        "id": "7wS9HKFYLqzF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain import OpenAI, LLMChain, ConversationBufferMemory\n",
        "\n",
        "# Initialize the OpenAI client\n",
        "client = OpenAI(\n",
        "    base_url=\"https://integrate.api.nvidia.com/v1\",\n",
        "    api_key=\"nvapi-tlopcFnGdWAxQJ9wgoIG8cNJ5XGUO1dpYcvLyULNNdQXGUNxJ5b\"\n",
        ")\n",
        "\n",
        "# Define the model\n",
        "model_name = \"meta/llama3-70b-instruct\"\n",
        "\n",
        "# Initialize memory component\n",
        "memory = ConversationBufferMemory()\n",
        "\n",
        "# Create the LLM Chain with memory\n",
        "llm_chain = LLMChain(\n",
        "    llm=client,\n",
        "    prompt_template=\"You are an AI assistant. Continue the conversation below:\\n\\n{history}\\n\\nUser: {user_input}\\nAssistant:\",\n",
        "    memory=memory\n",
        ")\n",
        "\n",
        "def generate_response(user_input, chain):\n",
        "    response = chain.run(user_input)\n",
        "    return response\n",
        "\n",
        "# Example usage\n",
        "user_input = \"Write a limerick about the wonders of GPU computing.\"\n",
        "response = generate_response(user_input, llm_chain)\n",
        "print(\"Model:\", response)\n",
        "\n",
        "# Further interaction example\n",
        "user_input = \"Tell me more about the history of GPU computing.\"\n",
        "response = generate_response(user_input, llm_chain)\n",
        "print(\"Model:\", response)\n"
      ],
      "metadata": {
        "id": "52JRPD4bLeNF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2BKijbPAh3X4"
      },
      "outputs": [],
      "source": [
        "#output function\n",
        "def final_result(question):\n",
        "  docs = vdb_connection.similarity_search(question,k=3)\n",
        "  context = \"\\n\\n \".join([doc.page_content for doc in docs])\n",
        "\n",
        "  promp_template_for_processing = f\"Answer this Question ({question}) in an exhaustive and concise way, based only on the given context.\\n\\n\" \\\n",
        "+ f\"Context: {context}\\n\\n\" + f\"Question : {question}\\n\"\n",
        "\n",
        "  client = OpenAI(\n",
        "  base_url = \"https://integrate.api.nvidia.com/v1\",\n",
        "  api_key = \"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\"\n",
        ")\n",
        "\n",
        "  completion = client.chat.completions.create(\n",
        "    model=\"meta/llama3-70b-instruct\",\n",
        "    messages=[{\"role\":\"user\",\"content\":promp_template_for_processing}],\n",
        "    temperature=0.1,\n",
        "    top_p=1,\n",
        "    max_tokens=1024,\n",
        "    stream=True\n",
        "  )\n",
        "\n",
        "  response = \"\"\n",
        "  for chunk in completion:\n",
        "      if chunk.choices[0].delta.content is not None:\n",
        "          response += chunk.choices[0].delta.content\n",
        "  return display(Markdown(response))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ju2F-o2sh5C0"
      },
      "outputs": [],
      "source": [
        "while True:\n",
        "    question = input(\"Ask me a question? (Type 'exit' to end): \")\n",
        "    if question.lower() == \"exit\":\n",
        "        break\n",
        "    result = final_result(question)\n",
        "    result"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}